<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Learning Unified Multimodal Tactile Representations.">
  <meta name="keywords" content="Multimodal, touch, LLM, generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Binding Touch to Everything: Learning Unified Multimodal Tactile Representations</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Binding Touch to Everything: Learning Unified Multimodal Tactile
              Representations</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://fredfyyang.github.io/">Fengyu Yang*</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://cfeng16.github.io/">Chao Feng*</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://ificl.github.io/">Ziyang Chen*</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=A3c4pHkAAAAJ&hl=ko">Hyoungseob Park</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?email=daniel.wang.dhw33@yale.edu">Daniel Wang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://dou-yiming.github.io/">Yiming Dou</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://adonis-galaxy.github.io/homepage/">Ziyao Zeng</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?email=xien.chen@yale.edu">Xien Chen</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?email=rit.gangopadhyay@yale.edu">Rit
                  Gangopadhyay</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://andrewowens.com/">Andrew Owens</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://vision.cs.yale.edu/members/alex-wong.html">Alex Wong</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Yale University,</span>
              <span class="author-block"><sup>2</sup>University of Michigan</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- Replace the video tag with an image tag -->
        <img id="teaser" src="static/images/teaser.png" alt="Teaser Figure" style="width: 100%; height: auto;">

        <!-- <h2 class="subtitle has-text-centered"> -->
        <center>
          <p>
            Our UniTouch can solve a variety of sensing tasks, ranging from touch image understanding to image synthesis
            with touch, in zero-shot manners.
          </p>
        </center>
        <!-- </h2> -->
      </div>
    </div>
  </section>

  <section class="section"">
    <div class=" container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The ability to associate touch with other modalities has huge implications for humans and computational
            systems. However, multimodal learning with touch remains challenging due to the expensive data collection
            process and non-standardized sensor outputs. We introduce UniTouch, a unified tactile model for
            vision-based touch sensors connected to multiple modalities, including vision, language and sound. We
            achieve this by aligning our UniTouch embeddings to pretrained image embeddings already associated with a
            variety of other modalities. We further propose learnable sensor-specific tokens, allowing the model to
            learn from a set of heterogeneous tactile sensors, all at the same time. UniTouch is capable of conducting
            various touch sensing tasks in the zero-shot setting, from robot grasping prediction to touch image
            question and answering. To the best of our knowledge, UniTouch is the first to demonstrate such
            capabilities.
          </p>
        </div>
      </div>
    </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Method Overview</h2>
          <div class="content has-text-justified">
            <p>
              We align our touch embedding with a pre-trained image embedding derived from large-scale vision language
              data, using sensor-specific tokens for multi-sensor training.
            </p>
            <center>
              <img src="./static/images/method.png" alt="Overview" style="width:70%; height: auto;max-width: 768px;">
            </center>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">UniTouch Representation</h2>
          <div class="content has-text-justified">
            <center>
              <div style="display: flex; justify-content: center;">
                <div style="flex: 1; padding: 0px;">
                  <img src="./static/images/material_in_domain.png" alt="Image 1" style="width: 100%; height: auto;">
                  <p>
                    Material classification results on in-domain datasets.
                  </p>
                </div>
                <div style="flex: 1; padding: 0px;">
                  <img src="./static/images/material_ood.png" alt="Image 2" style="width: 100%; height: auto;">
                  <p>
                    Material classification results on out-of-domain datasets.
                  </p>
                </div>
              </div>
          </div>
          <div class="content has-text-justified">
            <center>
              <div style="display: flex; justify-content: center;">
                <div style="flex: 1; padding: 0px;">
                  <img src="./static/images/retrieval.png" alt="Image 1" style="width: 100%; height: auto;">
                  <p>
                    Cross-modal Retrieval Results.
                  </p>
                </div>
                <div style="flex: 1; padding: 0px;">
                  <img src="./static/images/grasping.png" alt="Image 2" style="width: 100%; height: auto;">
                  <p>
                    Robotics grasping stability prediction results.
                  </p>
                </div>
              </div>
          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Touch-LLM</h2>
          <div class="content has-text-justified">
            <p>
              We combine touch with large language models (LLM), allowing us to perform tasks such as tactile question
              answering in a variety of tactile domains, including contact localization, grasping stability prediction,
              and etc.
            </p>
            <center>
              <img src="./static/images/touch_llm.png" alt="Image 1" style="width: 100%; height: auto;">
              <!-- <video id="touch-llm" playsinline autoplay muted width="100%">
                <source src="./static/videos/touch_llm.mp4" type="video/mp4"> -->

              </video>
            </center>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Image synthesis with touch</h2>
        <div class="content has-text-justified">
        </div>
      </div>


      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h3 class="title is-4 has-text-centered" style="margin-top: -10px;">Touch to image generation</h3>
          <div class="content has-text-justified">
            <p>
              We show images generated solely from touch in a zero-shot manner. We use a pretrained text-to-image
              diffusion model, conditioned on our touch features.
            </p>
          </div>
          <center>
            <img src="./static/images/touch2img.png" alt="touch2img" style="width: 60%;">
          </center>
        </div>
      </div>
      <br />

      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h3 class="title is-4 has-text-centered" style="margin-top: -10px;">Touch driven image stylization</h3>
          <div class="content has-text-justified">
            <p>
              We show some zero-shot manipulated images by using off-the-shelf text-to-image diffusion model and touch
              signal.
          </div>
          <center>
            <img src="./static/images/tdis.png" alt="touch2img" style="width: 80%;">
          </center>
        </div>
      </div>

    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{yang2023binding,
      title={Binding Touch to Everything: Learning Unified Multimodal Tactile Representations},
      author={Yang, Fengyu and Feng, Chao and Chen, Ziyang and Park, Hyoungseob and Wang, Daniel and Dou, Yiming and Zeng, Ziyao and Chen, Xien and Gangopadhyay, Rit and Owens, Andrew and Wong, Alex},
      journal={arXiv 2023},
      year={2023},
    }</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              The website template comes from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>